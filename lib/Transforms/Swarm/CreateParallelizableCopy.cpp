//===- CreateParallelizableCopy.cpp - Copy parallelizable functions -------===//
//
//                       The SCC Parallelizing Compiler
//
//          Copyright (c) 2020 Massachusetts Institute of Technology
//
// This file is distributed under the University of Illinois Open Source
// License. See LICENSE.TXT for details.
//
//===----------------------------------------------------------------------===//
//
// Briefly: Identify functions as parallelizable or not.  If -auto-swarmify is
// passed, create a copy of each function that looks to be parallelizable.  The
// copy to be parallelized may accept a continuation as an additional
// argument, implementing one half of the transformation into
// continuation-passing style (CPS).  Also provide facilities for creating
// calls of these parallel functions and passing continuations to them,
// implementing the other half of the CPS transform.
//
// Long explanation:
//
// Ideally, SCC would transform every function with a return value into
// continuation-passing style (CPS).  Unfortunately, this would require
// recompiling every library used by our benchmark applications with SCC, and
// there are also practical implementation complexities to be solved with CPS
// transform for certain function calls (e.g., varadic functions).  In
// practice, we have found that library calls and difficult, complex function
// calls make up little of the runtime in the benchmarks we study.  So, we
// provide a form of backwards compatibility, where our parallelized code can
// safely call ordinarily compiled serial code.  When a parallelized task calls
// an ordinary serial function, the entire callee runs serially, within the
// task, until it returns normally.
//
// To do this, we enable parallelized code, which expects to invoke functions
// converted to CPS, to fall back to invoking the serial version of the callee.
// The key challenge here is that the callee may not be defined in the same
// translation unit, so the compiler does not know whether the callee will be
// parallelized or will exist only as an ordinary serial function.
// Furthermore, functions in other translation units or external libraries may
// depend upon being passed function pointers from parallel code, or finding
// the definitions of functions with certain names at link time, and expect to
// be able to call that function serially, without CPS.  (For example, external
// libraries may allow for extending their functionality by defining functions
// with certain names within the application.)  Thus, calls from external
// translation units not parallelized by the compiler must also find the
// original, unparallelized implementation of their callees.
//
// We solve this problem through two steps.  The first step is taken within the
// CreateParallelizableCopy pass: When we first identify a function that we
// will attempt to parallelize, we duplicate it, leaving an unmodified serial
// copy whose name is unchanged.  The duplicate of the function that we will
// parallelize is given a unique name formed from the original function name
// plus the suffix `.par`, which will never conflict with other global (i.e.,
// symbol table) names generated by Clang, since Clang does not generate names
// with periods in them.  Thus, any unmodified calls from serial code will find
// and call the unmodified serial copy of the code.  The copy with the `.par`
// suffix may have its function signature changed to take take one additional
// parameter, a possibly null pointer to a continuation closure, which, if it
// is not null, will be called and passed any return value the function
// produces when the return value is known.  After the CreateParallelizableCopy
// pass finishes, the copies with the `.par` suffix do not yet have any
// callers/users.
//
// The second step is to transform each function call in parallelized code to
// call the copy of its callee with the `.par` suffix.  Specifically, when
// parallelizing a function call, we must always transform the call to pass a
// continuation if the continuation makes use of a return value.  Otherwise, if
// we are not passing a continuation, we must still transform it to call the
// parallel copy of the callee with the correct signature, perhaps passing a
// null continuation.  This file provides the facilities to create the calls
// with the appropriate function signatures, which are used to create the calls
// during the Fractalization pass.  This file also provides a solution to the
// major difficulty in this step: We do not generally know if the callee even
// has a parallel version to call (e.g., if it's in another translation unit).
// If the parallel version exists, we must call it, but if it doesn't we must
// fall back to executing the serial version.  To accomplish this, we may
// generate a **weak definition** of the function with the `.par` suffix.
// A weak definition is one that will be used by the linker only if it
// cannot find an ordinary, strong definition of the function at linktime.
// Weak definitions are a standard feature supported by all modern object file
// formats and linkers.  We also refer to these weak definitions as
// [thunks](https://en.wikipedia.org/wiki/Thunk).
// The pseudocode below roughly illustrates the contents a thunk that we would
// generate for a call to an external function `foo()`.  For a function with no
// return value, the thunk simply calls the serial version:
// ```
// define foo.par( args... ):
//     foo( args... )
// ```
// With a return value, the thunk calls the serial version of the function
// (without the `.par` suffix), and waits until it returns before spawning the
// passed continuation, if any, to support callers expecting CPS:
// ```
// define foo.par( args... , continuation):
//     ret = foo( args... )
//     if (continuation):
//         spawn continuation(ret)
// ```
// The compiler generates parallelized calls that call this thunk.
// If an actual parallelied version of the callee exists, then the thunk will
// be discarded and the call will be resolved to call the real parallel callee
// at link time.  Otherwise, the call will invoke the thunk, wihch calls the
// serial version of the function (without the `.par` suffix).
//
// Note that the final compiled binary does not necessarily have multiple
// copies of every function, as preexisting capabilities in compilers and
// linkers can already discard code that is unneeded in the final executable
// binary.  When compiling one translation unit, the compiler automatically
// discards any original, unmodified serial functions with internal (i.e.,
// `static`) linkage that are unused because all of their callsites have been
// transformed to call the parallel copy.  Functions with external linkage can
// be discarded at link time, if the linker finds they are unused across the
// whole program.
//
// Finally, we need a way to handle applications that explicitly handle
// function pointers and to parallelize indirect function calls, which cannot
// be resolved at link time. In order to safely offer backwards compatibility
// with unmodified serial code that handles data structures that may contain
// function pointers, any code that takes the address of the function is left
// unmodified so that it uses the pointer to the original, serial version of
// the code.  We store a pointer to the parallel copy of every parallelized
// function immediately preceeding the entry point of the serial version.  When
// parallelizing a indirect function call, we can subtract a fixed offset from
// the serial function pointer to find the location in memory where the pointer
// to the parallel function is stored, and call the parallel copy of the
// function.
//
// Both GCC and LLVM normally align function entry points to multiples of 16
// bytes. By inserting a pointer to a parallelized function before the serial
// version, we have offset the entry point for serial functions that have
// parallel copies so that the entry points of these serial functions are no
// longer multiples of 16.  Therefore, before making a parallel indirect
// function call, we could simply insert a runtime check into the code to
// determine at runtime whether the the function pointer is a multiple of 16,
// in which case it should perform an ordinary serial function call and then
// run the continuation instead of passing it, or if it not a multiple of 16,
// it can find the pointer to the parallel version and call it with CPS.  We
// thus add the modest overhead of a branch and an extra indirection to
// indirect function calls, an acceptable price as indirect calls are rare in
// hot, performance-critical loops in the benchmarks we study.
//
//===----------------------------------------------------------------------===//

#include "CreateParallelizableCopy.h"

#include "Utils/Flags.h"
#include "Utils/Misc.h"
#include "Utils/Tasks.h"

#include "llvm/Analysis/OptimizationDiagnosticInfo.h"
#include "llvm/IR/Constants.h"
#include "llvm/IR/DebugInfoMetadata.h"
#include "llvm/IR/Function.h"
#include "llvm/IR/InstIterator.h"
#include "llvm/IR/IRBuilder.h"
#include "llvm/IR/PassManager.h"
#include "llvm/Support/Debug.h"
#include "llvm/Support/CommandLine.h"
#include "llvm/Transforms/Swarm.h"
#include "llvm/Transforms/Tapir/Outline.h"
#include "llvm/Transforms/Utils/BasicBlockUtils.h"

#include <vector>

using namespace llvm;

#define PC_NAME "parallelizable-copy"
#define DEBUG_TYPE PC_NAME

#define PARALLELIZERTHUNK ("SwarmParallelizerThunk")
#define PARALLELIZABLE_COPY ("SwarmParallelizableCopy")
#define PRESERVE_SERIAL_COPY ("SwarmSerialCopy")

static cl::opt<bool> AutoSwarmify("auto-swarmify",
        cl::init(false),
        cl::desc("Enables whole-program automatic speculative parallelization "
                 "including transforming function signatures to use CPS."));

namespace {

class CreateParallelizableCopy : public ModulePass {
public:
  static char ID;

  CreateParallelizableCopy() : ModulePass(ID) {
    initializeCreateParallelizableCopyPass(*PassRegistry::getPassRegistry());
  }

  bool runOnModule(Module &) override;
  void getAnalysisUsage(AnalysisUsage &) const override;
private:
  bool processFunction(Function &F);
};


class FunctionCopier {
  Function &F;
  OptimizationRemarkEmitter &ORE;

public:
  FunctionCopier(Function &F, OptimizationRemarkEmitter &ORE)
      : F(F), ORE(ORE)
  {}

  bool isPermitted() const;
  bool isSomethingWeShouldImplementButHavent() const;
  void createParallelizableCopy();

  static constexpr const char *ParallelFuncSuffix = ".par";
private:
  void debugNotParallelizable(const Twine &Reason) const;
  void warnNotParallelizable(StringRef RemarkName, const Twine &Reason,
                             const Instruction *Inst = nullptr) const;
  bool mayBeIndirectlyCalled(const Function &F) const;
};


} // end anonymous namespace


// Returns a dummy pointer type.
// We cannot statically know the type of each continuation closure struct,
// because a single parallel function may have many callers that each pass
// different continuation closures with different types of captured values.
static PointerType *getContClosurePtrTy(LLVMContext &Context) {
  return PointerType::getUnqual(Type::getInt8Ty(Context));
}


// Returns a function pointer type for the continuation passed by a caller
// that needs a value of type ReturnType from its callee. This is
// void (*)(ReturnType retValue, ContClosurePtrTy closure)
// or, if ReturnType is void, it is
// void (*)(ContClosurePtrTy closure)
// Note that we pass the entire closure object that holds both captured
// values and the continuation function pointer to the continuation function,
// because the continuation function is responsible for both extracting the
// captured values and also freeing the object.
static PointerType *getContFuncPtrTy(Type *ReturnType) {
  LLVMContext &Context = ReturnType->getContext();
  SmallVector<Type *, 2> ParamTys;
  if (!ReturnType->isVoidTy())
    ParamTys.push_back(ReturnType);
  ParamTys.push_back(getContClosurePtrTy(Context));
  return PointerType::getUnqual(
          FunctionType::get(Type::getVoidTy(Context), ParamTys,
                            /*isVarArgs=*/false));
}


static FunctionType *getCPSFunctionType(FunctionType *SerialType) {
  assert(SerialType && !SerialType->isVarArg() && "Var args not supported yet");
  std::vector<Type *> Params(SerialType->params());
  Params.push_back(getContClosurePtrTy(SerialType->getContext()));
  Type *RetTy = Type::getVoidTy(SerialType->getContext());
  return FunctionType::get(RetTy, Params, false);
}


static bool shouldConvertToCPS(FunctionType *SerialFuncType) {
  return !SerialFuncType->getReturnType()->isVoidTy();
}


/// Return the parallel version of SerialFuncType.
/// That is, return the type that a parallel version of a function would have
/// if the serial version of that function had type SerialFuncType.
static FunctionType *getParallelFunctionType(FunctionType *SerialFuncType) {
  return shouldConvertToCPS(SerialFuncType) ? getCPSFunctionType(SerialFuncType) : SerialFuncType;
}


// Insert a call to the continuation before RI.
// If RI returns a value, pass the value to the continuation
// and change RI to return void.
// Spawn the newly created continuation call to the right timestamp.
// In pseudocode, this replaces
//
//   return retval;
//
// with
//
//   if (continuation) {
//     spawn (swarm::timestamp() + 1) {
//       (*continuation)(retval, continuation);
//     }
//   }
//
// and if there was previously no return value, then this creates
//
//   if (continuation) {
//     spawn (swarm::timestamp() + 1) {
//       (*continuation)(continuation);
//     }
//   }
//
static void createContinuationSpawn(ReturnInst *RI,
                                    AttributeSet SeqReturnAttrs) {
  Function *F = RI->getFunction();
  Value *RetVal = RI->getReturnValue();

  if (RetVal) {
    // Don't actually return a value
    auto NewRI = ReturnInst::Create(F->getContext());
    ReplaceInstWithInst(RI, NewRI);
    RI = NewRI;
  }

  // Spawn the continuation, if any.
  assert(!F->arg_empty());
  Argument *ContClosure = std::prev(F->arg_end());
  IRBuilder<> Builder(RI);
  Value *IsNotNull = Builder.CreateIsNotNull(ContClosure);
  TerminatorInst *ThenTerm = SplitBlockAndInsertIfThen(IsNotNull, RI,
                                                       /*Unreachable=*/false);
  Builder.SetInsertPoint(ThenTerm);
  assert(ContClosure->getType() == getContClosurePtrTy(F->getContext()));
  PointerType *ContFuncPtrTy = getContFuncPtrTy(
          RetVal ? RetVal->getType()
                 : Type::getVoidTy(F->getContext()));
  DEBUG(dbgs() << "continuation function pointer type: "
               << *ContFuncPtrTy << "\n");
  // The function pointer is the first field of the closure struct
  auto ContFuncPtrPtr = cast<Instruction>(Builder.CreatePointerCast(
          ContClosure,
          PointerType::getUnqual(ContFuncPtrTy),
          "cont-funcptr-ptr"));
  LoadInst *ContFuncPtr = Builder.CreateLoad(ContFuncPtrPtr, "cont-funcptr");
  ContFuncPtr->setMetadata(SwarmFlag::Closure,
                           MDNode::get(F->getContext(), {}));
  SmallVector<Value *, 2> ContArgs;
  if (RetVal)
    ContArgs.push_back(RetVal);
  ContArgs.push_back(ContClosure);
  CallInst *CI = Builder.CreateCall(ContFuncPtr, ContArgs);
  assert(CI->getCallingConv() == CallingConv::C);
  if (RetVal) {
    if (SeqReturnAttrs.hasAttribute(Attribute::ZExt)) {
      CI->addParamAttr(ContArgs.size() - 2, Attribute::ZExt);
    } else if (SeqReturnAttrs.hasAttribute(Attribute::SExt)) {
      CI->addParamAttr(ContArgs.size() - 2, Attribute::SExt);
    }
  }

  SDetachInst *DI;
  detachTask(ContFuncPtrPtr, ThenTerm, Builder.getInt64(1), nullptr,
             DetachKind::CallPassedCont, F->getName() + ".contcall",
             nullptr, nullptr,
             &DI);
  DI->setRelativeTimestamp(true);
}


/// Create a callee-private (F-private) copy of Arg's struct which has the byval
/// attribute. Remove the byval attribute so that CodeGen doesn't also
/// stack-allocate the object. If we permit CodeGen to stack-allocate the copy,
/// then we'd observe different threads accessing each other's stacks, which is
/// forbidden.
static void replaceByValWithAlloca(Argument &Arg, Function &F) {
  IRBuilder<> B(&*F.getEntryBlock().getFirstInsertionPt());
  StructType *AT =
          cast<StructType>(cast<PointerType>(Arg.getType())->getElementType());

  // While it would seem we should malloc/free the Arg copy here, we can instead
  // create an AllocaInst because Fractalizer is responsible to replace
  // AllocaInsts with heap allocations. When Fractalizer gains memory
  // allocation sophistication, (e.g. accumulating all AllocaInsts into one
  // malloc call, or only heap-allocating objects known to cross task
  // boundaries) this byval feature will automatically benefit.
  auto *Alloca = B.CreateAlloca(AT, nullptr, Arg.getName() + ".byval_copy");
  Alloca->setAlignment(Arg.getParamAlignment());
  Arg.replaceAllUsesWith(Alloca);

  const DataLayout &DL = F.getParent()->getDataLayout();
  B.CreateMemCpy(Alloca, &Arg,
                 DL.getTypeAllocSize(AT),
                 Arg.getParamAlignment());

  Arg.removeAttr(Attribute::ByVal);
  // Because the Arg was passed ByVal, and we've now replaced all its uses with
  // a callee-private copy, then the argument does not alias with anything in
  // the function.
  // FIXME(mcj) SwarmABI doesn't want any NoAlias attributes
  // F.addParamAttr(Arg.getArgNo(), Attribute::NoAlias);
  // TODO(mcj) also add the NoAlias to the CallInsts?

  DEBUG(dbgs() << "  replaced " << F.getName()
               << "'s byval arg " << Arg
               << "\n  with callee-private Alloca copy\n  " << *Alloca
               << "\n");
}


static void replaceByValsWithAllocaCopies(Function &F) {
  for (Argument &Arg : F.args()) {
    if (Arg.hasByValAttr()) replaceByValWithAlloca(Arg, F);
  }
}


static void removeGuaranteeAttributes(Function &F) {
  F.removeFnAttr(Attribute::ReadNone);
  F.removeFnAttr(Attribute::ReadOnly);
  F.removeFnAttr(Attribute::WriteOnly);
  F.removeFnAttr(Attribute::ArgMemOnly);
  F.removeFnAttr(Attribute::InaccessibleMemOnly);
  F.removeFnAttr(Attribute::InaccessibleMemOrArgMemOnly);
  F.removeFnAttr(Attribute::Speculatable);
  F.removeFnAttr(Attribute::NoRecurse);

  for (Argument &Arg : F.args()) {
    Arg.removeAttr(Attribute::NoCapture);
    // TODO(mcj) Wouldn't it be safe to keep NoAlias for an argument? Daniel is
    // generally interested in exploiting the NoAlias attribute.
    Arg.removeAttr(Attribute::NoAlias);
    Arg.removeAttr(Attribute::Returned);

    // The sret and byval attributes are generated by the frontend to signify
    // when the target ABI requires specific placement for objects passed or
    // returned by value, usually on the stack.  For example, the System V
    // x86_64 ABI requires that, when passing or returning certain objects by
    // value that do not go in registers, the caller must allocate space for
    // the objects at particular standardized stack offsets, so that there is
    // no need to actually pass a pointer to this space.  The way to represent
    // this in LLVM IR is to pass a *fictitous* pointer argument with a byval
    // or sret attribute.  Standard middle-end optimization passes can then
    // optimize the loads and stores to this pointer, before the backend
    // eventually removes the pointer argument and instead computes the
    // location of the object as some particular offset on the stack.
    //
    // Since we generate all calls of parallelizable copies, we have no need to
    // maintain compatibility with any prior standard ABI.  It'd be nice to
    // transform srets and byvals to just pass and return all objects by value
    // directly, and let our lowering pass take its best shot at packing
    // everything into registers.  For now, we've gone for something easier to
    // implement: We just let Fractalizer move these allocations to the heap
    // and pass real pointers to the heap allocations.
    // See also:  replaceByValsWithAllocaCopies()
    Arg.removeAttr(Attribute::StructRet);
  }
}


static bool hasAnyGuaranteeAttribute(const CallInst &CI) {
  return (CI.hasFnAttr(Attribute::ReadNone) ||
          CI.hasFnAttr(Attribute::ReadOnly) ||
          CI.hasFnAttr(Attribute::WriteOnly) ||
          CI.hasFnAttr(Attribute::ArgMemOnly) ||
          CI.hasFnAttr(Attribute::InaccessibleMemOnly) ||
          CI.hasFnAttr(Attribute::InaccessibleMemOrArgMemOnly) ||
          CI.hasFnAttr(Attribute::Speculatable) ||
          CI.hasFnAttr(Attribute::NoRecurse) ||
          any_of(CI.getAttributes(), [] (const AttributeSet &AS) {
            return  AS.hasAttribute(Attribute::ByVal) ||
                    AS.hasAttribute(Attribute::NoCapture) ||
                    AS.hasAttribute(Attribute::NoAlias) ||
                    AS.hasAttribute(Attribute::Returned) ||
                    AS.hasAttribute(Attribute::StructRet);
          })
        );
}


static void copyABIImpactingArgAttributes(AttributeList From, CallInst *To) {
  for (unsigned i = 0; i < To->getNumArgOperands(); ++i) {
    if (From.hasParamAttr(i, Attribute::ZExt)) {
      assert(To->getArgOperand(i)->getType()->getIntegerBitWidth() < 32);
      To->addParamAttr(i, Attribute::ZExt);
    }
    if (From.hasParamAttr(i, Attribute::SExt)) {
      assert(To->getArgOperand(i)->getType()->getIntegerBitWidth() < 32);
      To->addParamAttr(i, Attribute::SExt);
    }
  }
}


static void copyCallSideArgAttributes(AttributeList From, CallInst *To) {
  LLVMContext &Ctx = To->getContext();

  // If the original (serial) CallInst could guarantee
  // * non-nullness of a pointer argument
  // * dereferenceability of a pointer argument
  // there's no reason the Swarm version cannot make the same guarantee
  for (unsigned i = 0; i < To->getNumArgOperands(); ++i) {
    if (From.hasParamAttr(i, Attribute::NonNull)) {
      assert(To->getArgOperand(i)->getType()->isPointerTy());
      To->addParamAttr(i, Attribute::NonNull);
    }
    if (unsigned Bytes = From.getParamDereferenceableBytes(i)) {
      assert(To->getArgOperand(i)->getType()->isPointerTy());
      To->addParamAttr(i, Attribute::getWithDereferenceableBytes(Ctx, Bytes));
      // It was surprisingly tricky to propagate these attributes, so here are
      // some assertions as a gift to quickly tell if the attributes are wrong
      assert(To->paramHasAttr(i, Attribute::Dereferenceable));
      assert(To->getAttributes().getParamDereferenceableBytes(i) == Bytes);
    }
    if (unsigned Bytes = From.getParamDereferenceableOrNullBytes(i)) {
      assert(To->getArgOperand(i)->getType()->isPointerTy());
      To->addParamAttr(i, Attribute::getWithDereferenceableOrNullBytes(Ctx,
                                                                       Bytes));
      assert(Bytes == To->getAttributes()
                         .getParamDereferenceableOrNullBytes(i));
    }
    if (unsigned Bytes = From.getParamAlignment(i)) {
      assert(To->getArgOperand(i)->getType()->isPointerTy());
      To->addParamAttr(i, Attribute::getWithAlignment(Ctx, Bytes));
      assert(To->getParamAlignment(i) == Bytes);
    }
  }
}


Function *llvm::getOrInsertParallelVersion(Function *SF,
                                           AttributeList CallAttributes) {
  assert(!SF->isIntrinsic());
  assert(!SF->hasFnAttribute(SwarmFlag::Parallelizable)
         && !SF->hasFnAttribute(SwarmFlag::Parallelized)
         && "The serial copy should be deemed not parallelizable");
  assert(!SF->getName().contains(FunctionCopier::ParallelFuncSuffix));

  // If not auto-swarmifying, there is no separate parallel version.
  if (!AutoSwarmify)
    return SF;

  SmallString<256> FNameStr(SF->getName());

  // Special handling for libwrapmain.
  if (FNameStr.equals("__real_main"))
    FNameStr = "main";

  FNameStr += FunctionCopier::ParallelFuncSuffix;
  FunctionType *SFTy = SF->getFunctionType();
  // Fractalizer::cannotSpawn() ensures that the only time we spawn a LLVM
  // varargs call is for C functions with no prototype and no arguments,
  // so we can switch to a non-variadic calling convention.
  bool FalseVarArg = SFTy->isVarArg();
  if (FalseVarArg) {
    assert(!SFTy->getNumParams());
    SFTy = FunctionType::get(SFTy->getReturnType(), false);
  }
  FunctionType *PFTy = getParallelFunctionType(SFTy);
  assert(PFTy);
  Module *M = SF->getParent();
  Function *PF = cast<Function>(M->getOrInsertFunction(FNameStr, PFTy));

  if (PF->empty()) {
    // Parallelizable version doesn't exist yet.
    // We want to use the parallelizable version if it is generated later
    // in this translation unit, or if it is generated in another
    // translation unit. But if no parallel version is ever generated,
    // we need to safely call the sequential version.
    DEBUG(dbgs() << "Creating a weak thunk for " << FNameStr << "()\n");

    PF->copyAttributesFrom(SF);  // sets calling convention and other stuff.
    // Remove return attributes, since PF returns void.
    PF->setAttributes(SF->getAttributes().removeAttributes(
            PF->getContext(), AttributeList::ReturnIndex));
    removeGuaranteeAttributes(*PF);
    // This is weak linkage, so it will be replaced by the parallel version
    // if it is provided in another translation unit.
    PF->setLinkage(GlobalValue::LinkOnceAnyLinkage);
    assert(PF->isInterposable() &&
           "The compiler should NOT assume this is the final definition"
           "e.g., it must not inline this thunk into its callers");
    assert(PF->isDiscardableIfUnused() &&
           "Each module shall have its own thunks to use, created on demand");

    auto *Entry = BasicBlock::Create(M->getContext(), "entry", PF);
    IRBuilder<> Builder(Entry);
    SmallVector<Value *, 16> Args;
    for (Argument &Arg : PF->args())
      Args.push_back(&Arg);
    const bool ConvertToCPS = shouldConvertToCPS(SFTy);
    if (ConvertToCPS) {
      assert(Args.back()->getType() == getContClosurePtrTy(M->getContext()));
      Args.pop_back();
    }
    CallInst *TCI = Builder.CreateCall(SF, Args);
    TCI->setTailCall();
    TCI->setCallingConv(SF->getCallingConv());
    TCI->setAttributes(SF->getAttributes());
    copyABIImpactingArgAttributes(CallAttributes, TCI);
    ReturnInst *RI = SF->getReturnType()->isVoidTy()
                     ? Builder.CreateRetVoid()
                     : Builder.CreateRet(TCI);
    AttributeSet ReturnAttrs = SF->getAttributes().getRetAttributes();
    if (ConvertToCPS)
      createContinuationSpawn(RI, ReturnAttrs);
    // The thunk and the actual parallelizable copy must have the same signature
    // and calling convention so that a call to one can be substituted by the
    // linker into a call to the other
    replaceByValsWithAllocaCopies(*PF);

    PF->addFnAttr(PARALLELIZERTHUNK);
    assertVerifyFunction(*PF, "Created thunk");
  }

  assert(PF->hasFnAttribute(SwarmFlag::Parallelized)
         || PF->hasFnAttribute(SwarmFlag::Parallelizable)
         || PF->hasFnAttribute(SwarmFlag::Parallelizing) // recursive call
         || PF->hasFnAttribute(PARALLELIZERTHUNK)
         && "Found serial function with name indicating parallel version?");

  return PF;
}


Instruction *llvm::createHasParVersion(Value *SerialCalleePtr,
                                       Instruction *InsertBefore) {
  assert(cast<PointerType>(SerialCalleePtr->getType())
             ->getElementType()->isFunctionTy());
  assert(!isa<Function>(SerialCalleePtr) && "Must be an indirect call");
  assert(!isa<Constant>(SerialCalleePtr) && "Must be an indirect call");
  const DataLayout &DL = InsertBefore->getModule()->getDataLayout();
  IRBuilder<> Builder(InsertBefore);
  Value *PtrAddress = Builder.CreatePtrToInt(
      SerialCalleePtr, Builder.getIntNTy(DL.getPointerSizeInBits()),
      "serial_callee_addr");
  // We assume normal functions are aligned to 16 bytes, so the least
  // significant four bits of a function pointer would normally be zero.
  // However, the serial copy of a parallelized function will have a pointer to
  // the parallelized version as prefix data, offsetting the serial copy's
  // entrypoint so at least one of the bottom four bits in the serial function
  // pointer are set.
  assert(DL.getPointerSize() & 0xF);
  return cast<Instruction>(Builder.CreateIsNotNull(
      Builder.CreateAnd(PtrAddress, 0xF), "has_par_version"));
}


Instruction *llvm::createGetParFuncPtr(Value *SerialCalleePtr,
                                       Instruction *InsertBefore) {
  assert(!isa<Function>(SerialCalleePtr) && "Must be an indirect call");
  // This strategy of casting the pointer and dereferencing index -1
  // is informed by: https://llvm.org/docs/LangRef.html#prefix-data
  IRBuilder<> Builder(InsertBefore);
  auto *SerialFuncTy = cast<FunctionType>(
      cast<PointerType>(SerialCalleePtr->getType())->getElementType());
  Type *ParFuncTy = getParallelFunctionType(SerialFuncTy);
  ParFuncTy = PointerType::getUnqual(ParFuncTy);
  ParFuncTy = PointerType::getUnqual(ParFuncTy);
  Value *ParFuncPtrPtr = Builder.CreateConstGEP1_32(
      Builder.CreateBitCast(SerialCalleePtr, ParFuncTy), -1,
      "par_func_ptr_ptr");
  return Builder.CreateLoad(ParFuncPtrPtr, "par_func_ptr");
}


CallInst *llvm::createParallelCall(Value *PF,
                                   CallInst *SCI,
                                   Value *ContClosure,
                                   Instruction *InsertBefore) {
  // If not auto-swarmifying, there is no separate parallel version.
  if (!AutoSwarmify) {
    assert(!ContClosure && "We only do CPS conversion with -auto-swarmify");
    assert(PF = SCI->getCalledValue());
    SCI->setMetadata(SwarmFlag::ParallelCall, MDNode::get(PF->getContext(), {}));
    return SCI;
  }

  SmallVector<Value *, 16> Args(SCI->arg_operands());
  IRBuilder<> Builder(InsertBefore);
  const bool ConvertToCPS = shouldConvertToCPS(SCI->getFunctionType());
  if (ConvertToCPS) {
    PointerType *ContClosurePtrTy = getContClosurePtrTy(PF->getContext());
    Args.push_back(ContClosure ? Builder.CreatePointerCast(ContClosure,
                                                           ContClosurePtrTy,
                                                           "cont-closure-ptr")
                               : ConstantPointerNull::get(ContClosurePtrTy));
  } else {
    assert(!ContClosure);
  }
  assert(Args.size() == cast<FunctionType>(
          cast<PointerType>(PF->getType())->getElementType()
                                          )->getNumParams());
  CallInst *PCI = Builder.CreateCall(PF, Args);
  PCI->setCallingConv(SCI->getCallingConv());
  if (SCI->getDebugLoc()) PCI->setDebugLoc(SCI->getDebugLoc());
  copyABIImpactingArgAttributes(SCI->getAttributes(), PCI);
  copyCallSideArgAttributes(SCI->getAttributes(), PCI);
  assert(!hasAnyGuaranteeAttribute(*PCI) &&
         "The parallel call should not offer any guarantees");
  SCI->setMetadata(SwarmFlag::ParallelCall, MDNode::get(PF->getContext(), {}));
  return PCI;
}


// A simple way to recursively search def-use chains.
static bool isFunctionUsedBy(const Value &V, const Value &Target) {
  SmallPtrSet<const Value *, 8> Visiting;

  std::function<bool(const Value &)> RecursiveHelper = [&] (const Value &V) {
    if (Visiting.count(&V))
      return false;
    Visiting.insert(&V);
    const auto I = dyn_cast<Instruction>(&V);
    const auto Users = I ? I->getFunction()->users()
                         : V.users();
    for (const User *U : Users) {
      const Value *UV = cast<Value>(U);
      if (UV == &Target)
        return true;
      if (RecursiveHelper(*UV))
        return true;
    }
    return false;
  };

  return RecursiveHelper(V);
}


bool FunctionCopier::isPermitted() const {
  // It is the caller's job to ensure it isn't asking about functions
  // already processed or generated by CPC
  assert(!F.hasFnAttribute(PARALLELIZABLE_COPY));
  assert(!F.hasFnAttribute(PRESERVE_SERIAL_COPY));
  assert(!F.hasFnAttribute(PARALLELIZERTHUNK));
  assert(!F.hasFnAttribute(SwarmFlag::Parallelizable));
  assert(!F.hasFnAttribute(SwarmFlag::Parallelizing));
  assert(!F.hasFnAttribute(SwarmFlag::Parallelized));
  assert(!F.getName().contains(".outline"));
  assert(!F.getName().contains(".par"));

  if (!AutoSwarmify && !F.hasFnAttribute(SwarmAttr::Swarmify)) {
    debugNotParallelizable(
            "due to lack of -auto-swarmify or __attribute__((swarmify))");
    return false;
  }

  if (F.hasFnAttribute(SwarmAttr::NoSwarmify)) {
    debugNotParallelizable("due to __attribute__((noswarmify))");
    return false;
  }

  if (!AutoSwarmify && !F.getReturnType()->isVoidTy()) {
    debugNotParallelizable(
            "as it returns a value despite __attribute__((swarmify))");
    return false;
  }

  StringRef FName = F.getName();

  assert(!FName.startswith("__sccrt_") &&
         "An SCCRT function should be impossible to copy");

  // Avoid autoparallelizing Swarm runtime implementation stuff.
  if (FName.startswith("_ZN5swarm") || FName.startswith("_ZN3pls")) {
    debugNotParallelizable("as it is part of the Swarm runtime");
    return false;
  }

  // Avoid messing with some particular c++abi stuff.
  if (FName == "__clang_call_terminate") {
    debugNotParallelizable("as it is part of the C++ ABI");
    return false;
  }

  Module* M = F.getParent();

  // Avoid autoparallelizing global object constructors/destructors,
  // which run outside of main().
  if (auto GlobalConstructors = M->getNamedGlobal("llvm.global_ctors")) {
    if (isFunctionUsedBy(F, *GlobalConstructors)) {
      if (F.getSubprogram() && !F.getSubprogram()->getLine()) {
        debugNotParallelizable("due to use as global constructor");
      } else {
        warnNotParallelizable("Constructor",
                              "due to use as global constructor");
      }
      return false;
    }
  }
  if (auto GlobalDestructors = M->getNamedGlobal("llvm.global_dtors")) {
    if (isFunctionUsedBy(F, *GlobalDestructors)) {
      if (F.getSubprogram() && !F.getSubprogram()->getLine()) {
        debugNotParallelizable("due to use as global destructor");
      } else {
        warnNotParallelizable("Destructor", "due to use as global destructor");
      }
      return false;
    }
  }

  if (!getUniqueReturnInst(F)) {
    warnNotParallelizable("NoReturnBlock",
                          "due to its lack of a unique return instruction");
    return false;
  }

  if (any_of(F, [](const BasicBlock &BB) {
        auto *DI = dyn_cast<SDetachInst>(BB.getTerminator());
        return DI && DI->hasTimestamp();
      })) {
    warnNotParallelizable("AlreadyParallel",
                          "due to existing spawns/detaches with timestamps");
    return false;
  }

  assert(none_of(F, [] (const BasicBlock &BB) {
        const TerminatorInst *TI = BB.getTerminator();
        return (isa<DetachInst>(TI) && !isa<SDetachInst>(TI))
               || (isa<ReattachInst>(TI) && !isa<SReattachInst>(TI))
               || isa<SyncInst>(TI);
        }));

  return true;
}


static TerminatorInst *getAnyUnhandledExceptionHandlingTerminator(Function &F) {
  for (BasicBlock &BB : F) {
    TerminatorInst *TI = BB.getTerminator();
    if (isa<CatchReturnInst>(TI)
        || isa<CatchSwitchInst>(TI)
        || isa<CleanupReturnInst>(TI)
        || isa<InvokeInst>(TI)
        || isa<ResumeInst>(TI)
        ) {
      return TI;
    }
  }
  return nullptr;
}


static const AllocaInst *getAnyAllocaOutsideEntryBlock(const Function &F) {
  const BasicBlock &EntryBB = F.getEntryBlock();
  for (const BasicBlock &BB : F) {
    if (&BB != &EntryBB) {
      for (const Instruction &I : BB) {
        if (const auto *AI = dyn_cast<AllocaInst>(&I)) return AI;
      }
    }
  }
  return nullptr;
}


bool FunctionCopier::isSomethingWeShouldImplementButHavent() const {
  if (F.isVarArg()) {
    warnNotParallelizable("Variadic", "due to varargs");
    return true;
  }

  if (F.callsFunctionThatReturnsTwice()) {
    warnNotParallelizable("CallsSetjmp", "due to call of setjmp-like function");
    return true;
  }

  TerminatorInst *EHTerminator = getAnyUnhandledExceptionHandlingTerminator(F);
  assert((EHTerminator ||
          none_of(F, [] (const BasicBlock &BB) { return BB.isEHPad(); })) &&
         "We expect that a function with an exception handling pad has "
         "an exception handler terminator instruction");
  if (EHTerminator) {
    warnNotParallelizable("BadTerminator",
                          "due to exception handling terminator", EHTerminator);
    DEBUG(dbgs() << "Exception handling terminator: " << *EHTerminator << '\n');
    return true;
  }

  if (const AllocaInst *AI = getAnyAllocaOutsideEntryBlock(F)) {
    warnNotParallelizable("DynamicAlloca",
                          "due to an AllocaInst outside the entry block", AI);
    return true;
  }

  //victory: While many attributes merely describe analysis information that
  // is safe to drop, some have implications for the function calling ABI.
  // Some, in particular, will eventually be lowered to pass values by copying
  // them to the stack, which is a huge problem for us as we need to move
  // values that might be referenced across multiple tasks to the heap.
  // Others have attributes I don't fully understand, so I'm conservatively
  // blacklisting them just to be safe.
  const AttributeList AL = F.getAttributes();
  for (unsigned i = 0; i < F.arg_size(); i++) {
    for (const Attribute &Attr : AL.getParamAttributes(i)) {
      if (Attr.isStringAttribute()) {
        llvm_unreachable("parameter has string attribute?");
      } else {
        switch (Attr.getKindAsEnum()) {
        // These guarantee attributes should be harmless or handled by
        // removeGuaranteeAttributes()
        case Attribute::Alignment:
        case Attribute::NoAlias:
        case Attribute::Dereferenceable:
        case Attribute::DereferenceableOrNull:
        case Attribute::NoCapture:
        case Attribute::NonNull:
        case Attribute::ReadNone:
        case Attribute::ReadOnly:
        case Attribute::WriteOnly:
        case Attribute::Returned:
        case Attribute::StructRet:
        // These ABI-related attributes should be handled correctly
        // by copyABIImpactingArgAttributes()
        case Attribute::SExt:
        case Attribute::ZExt:
        // ByVal is an ABI-impacting attribute that we remove via a
        // callee-private copy of the arg in replaceByValsWithAllocaCopies()
        case Attribute::ByVal:
          continue;
        case Attribute::InAlloca:
        case Attribute::InReg:
        case Attribute::Nest:
        case Attribute::SwiftError:
        case Attribute::SwiftSelf:
        {
          const Argument &Arg = F.arg_begin()[i];
          warnNotParallelizable("ParameterAttr",
                  "as parameter " + Arg.getName() +
                  " has bad attribute " + Attr.getAsString());
          return true;
        }
        default:
          // victory: the list above documents all parameter attributes
          // https://llvm.org/docs/LangRef.html#parameter-attributes
          llvm_unreachable("Unrecognized attribute");
        }
      }
    }
  }
  for (const Attribute &Attr : AL.getRetAttributes()) {
    if (Attr.isStringAttribute()) {
      llvm_unreachable("return has string attribute?");
    } else {
      switch (Attr.getKindAsEnum()) {
        // The following attributes should be safe to drop.
        case Attribute::Alignment:
        case Attribute::NoAlias:
        case Attribute::Dereferenceable:
        case Attribute::DereferenceableOrNull:
        case Attribute::NonNull:
        // SExt or ZExt return values are handled in createContinuationSpawn
        case Attribute::SExt:
        case Attribute::ZExt:
          continue;
        default:
          warnNotParallelizable("ReturnAttr",
                  " as return has bad attribute " + Attr.getAsString());
          return true;
      }
    }
  }

  assert(!F.hasSection() &&
         "Parallelizable candidate is placed in a special section?");

  return false;
}


static Function *cloneIntoParallelForm(Function &F) {
  std::vector<BasicBlock *> BBs;
  for (BasicBlock &BB : F)
    BBs.push_back(&BB);
  assert(BBs[0] == &F.getEntryBlock());
  SetVector<Value *> Inputs, Outputs;
  for (Argument &Arg : F.args())
    Inputs.insert(&Arg);
  Instruction *ContParam = createDummyValue(getContClosurePtrTy(F.getContext()),
                                            "cont-closure",
                                            F.getEntryBlock().getTerminator());
  const bool ConvertToCPS = shouldConvertToCPS(F.getFunctionType());
  if (ConvertToCPS)
    Inputs.insert(ContParam);

  ValueToValueMapTy VMap;
  if (DISubprogram *SP = F.getSubprogram()) {
    // If we have debug info, add mapping for the metadata nodes that should not
    // be cloned by CloneFunctionInto.
    auto &MD = VMap.MD();
    MD[SP->getUnit()].reset(SP->getUnit());
    MD[SP->getType()].reset(SP->getType());
    MD[SP->getFile()].reset(SP->getFile());
  }
  SmallVector<ReturnInst *, 4> Returns;
  // All Swarm tasks must have void return type. The parallelizable variant of F
  // returns values (if any) by enqueuing a continuation.
  Type *RetTy = Type::getVoidTy(F.getContext());
  Function *ClonedF = CreateHelper(Inputs, Outputs, BBs,
                                   &F.getEntryBlock(), nullptr, nullptr,
                                   VMap, F.getParent(),
                                   F.getSubprogram() != nullptr, Returns,
                                   StringRef(),
                                   nullptr, nullptr,
                                   nullptr, nullptr, nullptr,
                                   RetTy);
  assert(ClonedF && "could not clone function");
  assert(ClonedF->getCallingConv() == F.getCallingConv());
  assert(!ClonedF->getAttributes().getRetAttributes().hasAttributes());

  cast<Instruction>(VMap[ContParam])->eraseFromParent();
  ContParam->eraseFromParent();

  ClonedF->setLinkage(F.getLinkage());

  if (ConvertToCPS) {
    ClonedF->addParamAttr(
        static_cast<unsigned>(ClonedF->arg_size() - 1),
        Attribute::get(F.getContext(), SwarmFlag::Continuation));

    ReturnInst *RI = getUniqueReturnInst(*ClonedF);
    DEBUG(dbgs() << "Using return instruction to create call to continuation:\n"
                 << *RI << "\n");
    AttributeSet ReturnAttrs = F.getAttributes().getRetAttributes();
    createContinuationSpawn(RI, ReturnAttrs);
  }

  ClonedF->addFnAttr(PARALLELIZABLE_COPY);

  return ClonedF;
}


// Leave the original function untouched and sequential. Make a copy to be
// parallelized by Fractalizer et al.
void FunctionCopier::createParallelizableCopy() {
  Module &M = *F.getParent();
  SmallString<256> FNameStr(F.getName());
  DEBUG(dbgs() << "Cloning " << FNameStr << "()\n");
  DEBUG(dbgs() << " Serial function has type " << *F.getFunctionType() << '\n');

  assert(!F.hasFnAttribute(SwarmFlag::Parallelized));
  assert(!F.hasFnAttribute(SwarmFlag::Parallelizable));

  // Create the new parallel version
  Function *PF = cloneIntoParallelForm(F);
  DEBUG(dbgs() << " Parallel function " << PF->getName() << "() has type "
               << *PF->getFunctionType() << '\n');
  assert(PF->getFunctionType() == getParallelFunctionType(F.getFunctionType()));
  removeGuaranteeAttributes(*PF);
  replaceByValsWithAllocaCopies(*PF);
  F.removeFnAttr(SwarmAttr::AssertSwarmified);
  F.addFnAttr(PRESERVE_SERIAL_COPY);

  // For safety, any invocations of FNameStr from other compilation units
  // should default to the sequential version.
  assert(F.getName() == FNameStr);
  assert(M.getNamedValue(FNameStr) == &F);
  assert(PF->user_empty());

  // Put the parallel version in the symbol table under the right name.
  assert(!FNameStr.endswith(ParallelFuncSuffix));
  FNameStr += ParallelFuncSuffix;
  assert(!M.getNamedValue(FNameStr));
  PF->setName(FNameStr);
  PF->addFnAttr(SwarmFlag::Parallelizable);
  assert(PF->getName() == FNameStr);
  assert(M.getNamedValue(FNameStr) == PF);

  if (mayBeIndirectlyCalled(F)) {
    DEBUG(dbgs() << "Storing pointer to " << PF->getName()
                 << "() as prefix data of " << F.getName() << "().\n");
    assert(!F.hasPrefixData());
    // We intentionally don't align to 16 bytes here,
    // See top-of-file comment, as well as llvm::createHasParVersion().
    F.setPrefixData(PF);
  }

  assertVerifyFunction(*PF, "Cloned parallelizable function");
}


void FunctionCopier::debugNotParallelizable(const Twine &Reason) const {
  DEBUG(dbgs() << "Function " << F.getName() << "() is not parallelizable "
               << Reason << ".\n");
}


void FunctionCopier::warnNotParallelizable(StringRef RemarkName,
                                           const Twine &Reason,
                                           const Instruction *Inst) const {
  std::string Msg;
  raw_string_ostream OS(Msg);
  OS << "Function " << F.getName() << "() is not parallelizable "
     << Reason << ".\n";
  OS.flush();
  ORE.emit(DiagnosticInfoOptimizationFailure(PC_NAME, RemarkName,
               Inst ? DiagnosticLocation(Inst->getDebugLoc())
                    : DiagnosticLocation(F.getSubprogram()),
               Inst ? Inst->getParent() : &F.getEntryBlock())
           << Msg);
}


/// \returns true if F may have its address taken internally or externally.
/// This is a lot like Function::hasAddressTaken(), except that it returns true
/// for linkage types that participate in linkage and can be used to resolve
/// external symbol references, and it looks closely at how the function is
/// used and prints some information.
bool FunctionCopier::mayBeIndirectlyCalled(const Function &F) const {
  bool ret = !F.hasLocalLinkage() && !F.hasAvailableExternallyLinkage();
  for (const Use &U : F.uses()) {
    if (auto *I = dyn_cast<Instruction>(U.getUser())) {
      if (CallSite(I) && CallSite(I).isCallee(&U)) {
        DEBUG(dbgs() << "function " << F.getName() << "()"
                     << " called by instruction " << *I << " in function "
                     << I->getFunction()->getName() << "()\n");
      } else {
        ORE.emit(OptimizationRemarkAnalysis(PC_NAME, "LocalFunctionPtr", I)
                 << "Pointer to function " << F.getName() << "()"
                 << " used other than by calling it in function "
                 << I->getFunction()->getName() << "()\n");
        DEBUG(dbgs() << " used by instruction " << *I << '\n');
        ret = true;
      }
    } else if (const auto *C = dyn_cast<ConstantAggregate>(U.getUser())) {
      ORE.emit(OptimizationRemarkAnalysis(PC_NAME, "ConstFunctionPtr",
                                          F.getSubprogram(),
                                          &F.getEntryBlock())
               << "Pointer to function " << F.getName() << "()"
               << " used in constant aggregate " << C->getName());
      DEBUG(dbgs() << " used by constant " << *C << '\n');
      ret = true;
    } else if (const auto *GV = dyn_cast<GlobalVariable>(U.getUser())) {
      ORE.emit(OptimizationRemarkAnalysis(PC_NAME, "GlobalFunctionPtr",
                                          F.getSubprogram(),
                                          &F.getEntryBlock())
               << "Pointer to function " << F.getName() << "()"
               << " used to initialize global variable " << GV->getName());
      DEBUG(dbgs() << " used by global variable " << *GV << '\n');
      assert(GV->hasInitializer());
      assert(GV->getInitializer() == &F);
      ret = true;
    } else if (const auto *C = dyn_cast<ConstantExpr>(U.getUser())) {
      ORE.emit(OptimizationRemarkAnalysis(PC_NAME, "CastFunctionPtr",
                                          F.getSubprogram(),
                                          &F.getEntryBlock())
               << "Pointer to function " << F.getName() << "()"
               << " used by casting it");
      DEBUG(dbgs() << " used by constant expression " << *C << '\n');
      assert(C->isCast());
      ret = true;
    } else if (const auto *A = dyn_cast<GlobalAlias>(U.getUser())) {
      //TODO(victory): Handle aliases somehow (parallel copies?).
      // This is a potential problem down the line.
      ORE.emit(DiagnosticInfoOptimizationFailure(PC_NAME, "AliasFunctionPtr",
                                                 F.getSubprogram(),
                                                 &F.getEntryBlock())
               << "Pointer to function " << F.getName() << "()"
               << " used as alias " << A->getName() << "()");
      DEBUG(dbgs() << " used by alias " << *A << '\n');
      ret = true;
    } else {
      dbgs() << "function " << F.getName() << "()"
             << " used by " << *U.getUser() << '\n';
      llvm_unreachable("Unexpected use type?");
    }
  }
  return ret;
}


bool CreateParallelizableCopy::runOnModule(Module &M) {
  if (skipModule(M))
    return false;

  bool AnyChanged = false;
  for (Function &F : M) {
    AnyChanged |= processFunction(F);
  }

  return AnyChanged;
}


bool CreateParallelizableCopy::processFunction(Function &F) {
  assert(!(AutoSwarmify && F.hasFnAttribute(SwarmAttr::Swarmify)) &&
         "Unexpected Swarmify attribute during automatic parallelization");
  if (F.isDeclaration())
    return false;

  if (F.hasFnAttribute(PRESERVE_SERIAL_COPY)
      || F.hasFnAttribute(PARALLELIZERTHUNK)
      || F.hasFnAttribute(SwarmFlag::Parallelizable)
      || F.hasFnAttribute(SwarmFlag::Parallelized)) {
    // Reencountering CPC-processed code
    assert(!F.hasFnAttribute(SwarmFlag::Parallelizable)
           || F.getName().endswith(FunctionCopier::ParallelFuncSuffix));
    assert(hasValidSwarmFlags(F));
    return false;
  }

  // See top-of-file comment, as well as llvm::createHasParVersion().
  assert(!F.hasPrefixData() && "Our approach to handling indirect calls"
         " assumes we're the only ones generating prefix data.");

  auto &ORE = getAnalysis<OptimizationRemarkEmitterWrapperPass>(F).getORE();
  FunctionCopier FC(F, ORE);

  if (!FC.isPermitted() || FC.isSomethingWeShouldImplementButHavent()) {
    assert(!errorIfAssertSwarmified(F) &&
           "Parallelization not permitted when programmer asserts success");
    assert(!F.hasFnAttribute(SwarmAttr::Swarmify) &&
           "Function tagged __attribute__((swarmify)) not Swarmifiable?");
    return false;
  }

  // Check if this is the lambda for the autoparallelized region of execution
  bool isROILambda = F.getName().contains("3scc17callROILambdaFunc");
  if (AutoSwarmify && !isROILambda) {
    FC.createParallelizableCopy();
  } else {
    assert(isROILambda || F.hasFnAttribute(SwarmAttr::Swarmify));
    F.addFnAttr(SwarmFlag::Parallelizable);
  }

  return true;
}


void CreateParallelizableCopy::getAnalysisUsage(AnalysisUsage &AU) const {
  AU.addRequired<OptimizationRemarkEmitterWrapperPass>();
}


char CreateParallelizableCopy::ID = 0;


INITIALIZE_PASS_BEGIN(CreateParallelizableCopy, PC_NAME,
                      "Copy functions that can be split into tasks",
                      false,
                      false /* analysis ... should be true? */)
INITIALIZE_PASS_DEPENDENCY(OptimizationRemarkEmitterWrapperPass)
INITIALIZE_PASS_END(CreateParallelizableCopy, PC_NAME,
                    "Copy functions that can be split into tasks",
                    false, false)

ModulePass *llvm::createCreateParallelizableCopyPass() {
  return new CreateParallelizableCopy();
}
